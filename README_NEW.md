# MNIST Image Classification Project

[![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/Python-3.8+-green.svg)](https://www.python.org/downloads/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)

A comprehensive machine learning and deep learning project for handwritten digit classification using the MNIST dataset. This repository implements end-to-end pipelines including data preprocessing, traditional ML models, NLP analysis, and CNN-based deep learning approaches.

## üìä Project Overview

This project demonstrates a complete ML/DL workflow:

1. **Data Preprocessing** - MNIST dataset loading, normalization, and splitting
2. **Traditional ML Models** - Decision Tree and Logistic Regression classifiers
3. **NLP Analysis** - Sentiment analysis using VADER and TextBlob (Task 3)
4. **Deep Learning** - Convolutional Neural Network for high-accuracy classification

### üéØ Key Results

| Model | Accuracy | Framework |
|-------|----------|-----------|
| Decision Tree | 86.90% | Scikit-learn |
| Logistic Regression | 92.01% | Scikit-learn |
| CNN | >95.00% | TensorFlow/Keras |

## üöÄ Quick Start

### Prerequisites

- Python 3.8 or higher
- pip or conda

### Installation

1. **Clone the repository:**
   ```bash
   git clone https://github.com/1511Darshan/image-classification.git
   cd image-classification
   ```

2. **Create and activate a virtual environment:**
   ```bash
   # Using venv
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   
   # OR using conda
   conda create -n mnist python=3.10
   conda activate mnist
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

## üìñ Usage Guide

### Training Models

#### Train all models:
```bash
python train.py
```

#### Train specific model:
```bash
python train.py --model decision-tree
python train.py --model logistic-regression
```

#### Train with custom output directory:
```bash
python train.py --output-dir ./results
```

### Making Predictions

#### Quick test (uses MNIST test set):
```bash
python predict.py --test
```

#### Predict on custom data:
```bash
python predict.py --model logistic-regression --input data.npy --output predictions.npy
```

### Running Tests

```bash
# Run all tests
pytest

# Run with verbose output
pytest -v

# Run specific test file
pytest tests/test_models.py

# Run with coverage
pytest --cov=src --cov-report=html
```

### Using in Jupyter Notebooks

See the `notebooks/` directory for interactive exploration:

```bash
jupyter notebook notebooks/
```

## üìÅ Project Structure

```
image-classification/
‚îú‚îÄ‚îÄ README.md                      # This file
‚îú‚îÄ‚îÄ LICENSE                        # MIT License
‚îú‚îÄ‚îÄ CONTRIBUTING.md               # Contribution guidelines
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ pytest.ini                    # Pytest configuration
‚îÇ
‚îú‚îÄ‚îÄ src/                          # Reusable Python modules
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ data.py                  # Data loading & preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ model.py                 # Model definitions & training
‚îÇ   ‚îî‚îÄ‚îÄ utils.py                 # Utility functions
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                    # Jupyter notebooks for exploration
‚îÇ   ‚îú‚îÄ‚îÄ task-1.ipynb             # Data preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ task-2.ipynb             # Traditional ML models
‚îÇ   ‚îú‚îÄ‚îÄ task-3.ipynb             # NLP sentiment analysis
‚îÇ   ‚îî‚îÄ‚îÄ task-4.ipynb             # Deep learning CNN
‚îÇ
‚îú‚îÄ‚îÄ tests/                        # Unit tests
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îî‚îÄ‚îÄ test_models.py           # Model tests
‚îÇ
‚îú‚îÄ‚îÄ data/                         # Data directory
‚îÇ   ‚îî‚îÄ‚îÄ README.md                # Dataset documentation
‚îÇ
‚îú‚îÄ‚îÄ models/                       # Saved model weights
‚îÇ   ‚îî‚îÄ‚îÄ .gitkeep
‚îÇ
‚îú‚îÄ‚îÄ train.py                      # Training script
‚îú‚îÄ‚îÄ predict.py                    # Prediction script
‚îÇ
‚îú‚îÄ‚îÄ X_train_scaled.npy           # Preprocessed training features
‚îú‚îÄ‚îÄ y_train.npy                  # Training labels
‚îú‚îÄ‚îÄ X_val.npy                    # Validation features
‚îú‚îÄ‚îÄ y_val.npy                    # Validation labels
‚îú‚îÄ‚îÄ X_test_scaled.npy            # Test features
‚îî‚îÄ‚îÄ y_test.npy                   # Test labels
```

## üìö Detailed Task Documentation

### Task 1: Data Preprocessing ‚úÖ

**File:** `notebooks/task-1.ipynb`

- Load MNIST dataset from binary IDX files
- Normalize pixel values to [0, 1]
- Standardize features (mean=0, std=1)
- Check for data quality and missing values
- Split into train/validation/test (80/20)
- Save as NumPy arrays

**Output:** `X_train_scaled.npy`, `y_train.npy`, `X_val.npy`, `y_val.npy`, `X_test_scaled.npy`, `y_test.npy`

### Task 2: Machine Learning Models ‚úÖ

**File:** `notebooks/task-2.ipynb` | **Reusable code:** `src/`

**Models Implemented:**

1. **Decision Tree Classifier**
   - Parameters: max_depth=30, min_samples_split=10, min_samples_leaf=5
   - Test Accuracy: 86.90%
   - Good for interpretability

2. **Logistic Regression**
   - Solver: lbfgs, Max iterations: 1000
   - Test Accuracy: 92.01%
   - Best traditional ML performance

**Evaluation Metrics:**
- Accuracy, Precision, Recall, F1-Score
- Confusion matrices
- Per-class performance analysis

### Task 3: Natural Language Processing ‚úÖ

**File:** `notebooks/task-3.ipynb`

- Sentiment analysis using VADER (Rule-based, optimized for social media)
- Sentiment analysis using TextBlob (Lexicon-based)
- Comparison of methods and subjectivity analysis
- Analysis of 20 sample texts

### Task 4: Deep Learning CNN ‚úÖ

**File:** `notebooks/task-4.ipynb`

**CNN Architecture:**
```
Input (28√ó28√ó1)
‚Üì
Conv2D(32) ‚Üí Conv2D(64) ‚Üí MaxPool ‚Üí Dropout(0.25)
‚Üì
Conv2D(128) ‚Üí MaxPool ‚Üí Dropout(0.25)
‚Üì
Dense(256) + BatchNorm + Dropout(0.5)
‚Üì
Dense(128) + BatchNorm + Dropout(0.3)
‚Üì
Dense(10) + Softmax
```

- **Optimizer:** Adam (learning_rate=0.001)
- **Loss:** Categorical Crossentropy
- **Training:** 30 epochs with Early Stopping
- **Test Accuracy:** >95.00%
- **Improvement over traditional ML:** +3-8%

## üî¨ Dataset Information

### MNIST Overview
- **Source:** [MNIST Database](http://yann.lecun.com/exdb/mnist/)
- **Samples:** 70,000 (60,000 training + 10,000 test)
- **Image Size:** 28√ó28 pixels (784 features)
- **Classes:** 10 (digits 0-9)
- **License:** Public Domain

See `data/README.md` for detailed dataset documentation and download instructions.

## üíª Development

### Code Style

This project follows Python best practices:

```bash
# Format code with black
black .

# Check with flake8
flake8 src/ tests/

# Run tests
pytest

# Check imports with isort
isort .
```

### Pre-commit Hooks

Set up pre-commit to automatically format/lint code:

```bash
pre-commit install
```

### Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md) for detailed contribution guidelines.

## üìä Model Performance Details

### Confusion Matrix Analysis

- **Decision Tree:** Performs well on digits 0, 1, 6, 8 but struggles with 4, 9
- **Logistic Regression:** More balanced performance across all digits
- **CNN:** Consistent >95% accuracy on all digit classes

### Learning Curves

Training history visualization shows:
- CNN: Smooth convergence, no overfitting
- Traditional ML: Immediate convergence

## üîß Configuration

### Python Dependencies

Core packages:
- `numpy>=1.24` - Numerical computing
- `pandas>=2.0` - Data manipulation
- `scikit-learn>=1.3` - Traditional ML
- `torch>=2.0` + `torchvision>=0.15` - Deep learning (PyTorch)
- `tensorflow>=2.14` - Alternative (TensorFlow/Keras)
- `matplotlib>=3.7`, `seaborn>=0.13` - Visualization

Development:
- `pytest>=7.4` - Testing
- `black>=23.7`, `flake8>=6.1` - Code quality
- `jupyter>=1.0` - Notebooks

See `requirements.txt` for complete list.

## üìà Results Summary

### Performance Metrics (Test Set)

| Metric | Decision Tree | Logistic Regression | CNN |
|--------|---------------|-------------------|-----|
| Accuracy | 86.90% | 92.01% | >95.00% |
| Precision | 0.8703 | 0.9205 | >0.95 |
| Recall | 0.8690 | 0.9201 | >0.95 |
| F1-Score | 0.8689 | 0.9200 | >0.95 |

### Key Insights

1. **Deep learning outperforms traditional ML** by 3-8% on image classification
2. **Logistic Regression** provides best speed/accuracy trade-off for traditional methods
3. **CNN architecture** with dropout and batch norm effectively prevents overfitting
4. **Data preprocessing** (standardization) crucial for all models

## üö® Troubleshooting

### Common Issues

**Issue:** Module not found error when running train.py
```bash
# Solution: Ensure you're in the project root directory
cd image-classification
python train.py
```

**Issue:** Data files not found
```bash
# Solution: Run task-1.ipynb first to generate preprocessed arrays
jupyter notebook notebooks/task-1.ipynb
```

**Issue:** CUDA/GPU errors with deep learning
```bash
# Solution: CPU-only mode is default. For GPU, reinstall PyTorch with CUDA support
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### Dataset License

The MNIST dataset is in the public domain. See [data/README.md](data/README.md) for citation details.

## üë• Author & Contact

**Darshan** ([@1511Darshan](https://github.com/1511Darshan))

For questions, issues, or suggestions:
- Open an [Issue](https://github.com/1511Darshan/image-classification/issues)
- Submit a [Pull Request](https://github.com/1511Darshan/image-classification/pulls)
- See [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines

## üîó References

- [MNIST Database](http://yann.lecun.com/exdb/mnist/)
- [Scikit-learn Documentation](https://scikit-learn.org)
- [PyTorch Documentation](https://pytorch.org/docs)
- [TensorFlow/Keras Documentation](https://www.tensorflow.org/guide)
- [Deep Learning for Computer Vision](https://cs231n.stanford.edu/)

## üéì Learning Resources

This project is suitable for:
- Learning ML fundamentals with real data
- Understanding data preprocessing pipelines
- Comparing traditional ML vs deep learning
- Best practices for code organization
- Testing and CI/CD in ML projects

---

## üìå Project Status

- ‚úÖ Task 1 (Data Preprocessing): Complete
- ‚úÖ Task 2 (Traditional ML Models): Complete  
- ‚úÖ Task 3 (NLP Analysis): Complete
- ‚úÖ Task 4 (Deep Learning CNN): Complete
- ‚úÖ Code Refactoring & Modularization: Complete
- ‚úÖ Testing & CI/CD: Implemented
- ‚úÖ Documentation: Complete

**Last Updated:** November 2025

---

<p align="center">
Made with ‚ù§Ô∏è for the ML community
</p>
