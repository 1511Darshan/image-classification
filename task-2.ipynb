{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6d8823d",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af35b55",
   "metadata": {},
   "source": [
    "## 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f502bffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4252e701",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240e9af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Dataset path\n",
    "base_path = r'C:\\Users\\rushd\\OneDrive\\Documents\\leetcode'\n",
    "\n",
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed MNIST dataset...\")\n",
    "X_train = np.load(os.path.join(base_path, 'X_train_scaled.npy'))\n",
    "y_train = np.load(os.path.join(base_path, 'y_train.npy'))\n",
    "X_val = np.load(os.path.join(base_path, 'X_val.npy'))\n",
    "y_val = np.load(os.path.join(base_path, 'y_val.npy'))\n",
    "X_test = np.load(os.path.join(base_path, 'X_test_scaled.npy'))\n",
    "y_test = np.load(os.path.join(base_path, 'y_test.npy'))\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"\\nDataset Shapes:\")\n",
    "print(f\"  Training: {X_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"  Validation: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "print(f\"  Test: {X_test.shape}, Labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e209a7ee",
   "metadata": {},
   "source": [
    "## 3. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaee98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Decision Tree Classifier\n",
    "print(\"Training Decision Tree Classifier...\")\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=30,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Decision Tree training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "dt_train_pred = dt_model.predict(X_train)\n",
    "dt_val_pred = dt_model.predict(X_val)\n",
    "dt_test_pred = dt_model.predict(X_test)\n",
    "\n",
    "print(\"‚úÖ Predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3512547b",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4c13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    random_state=42,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Logistic Regression training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "lr_train_pred = lr_model.predict(X_train)\n",
    "lr_val_pred = lr_model.predict(X_val)\n",
    "lr_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "print(\"‚úÖ Predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4711fe4e",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d34502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"DECISION TREE CLASSIFIER - PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training metrics\n",
    "dt_train_acc = accuracy_score(y_train, dt_train_pred)\n",
    "dt_val_acc = accuracy_score(y_val, dt_val_pred)\n",
    "dt_test_acc = accuracy_score(y_test, dt_test_pred)\n",
    "\n",
    "print(f\"\\nAccuracy Scores:\")\n",
    "print(f\"  Training Accuracy: {dt_train_acc:.4f} ({dt_train_acc*100:.2f}%)\")\n",
    "print(f\"  Validation Accuracy: {dt_val_acc:.4f} ({dt_val_acc*100:.2f}%)\")\n",
    "print(f\"  Test Accuracy: {dt_test_acc:.4f} ({dt_test_acc*100:.2f}%)\")\n",
    "\n",
    "# Detailed metrics on test set\n",
    "dt_precision = precision_score(y_test, dt_test_pred, average='weighted')\n",
    "dt_recall = recall_score(y_test, dt_test_pred, average='weighted')\n",
    "dt_f1 = f1_score(y_test, dt_test_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  Weighted Precision: {dt_precision:.4f}\")\n",
    "print(f\"  Weighted Recall: {dt_recall:.4f}\")\n",
    "print(f\"  Weighted F1-Score: {dt_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, dt_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a31ef2",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6825ffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION - PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training metrics\n",
    "lr_train_acc = accuracy_score(y_train, lr_train_pred)\n",
    "lr_val_acc = accuracy_score(y_val, lr_val_pred)\n",
    "lr_test_acc = accuracy_score(y_test, lr_test_pred)\n",
    "\n",
    "print(f\"\\nAccuracy Scores:\")\n",
    "print(f\"  Training Accuracy: {lr_train_acc:.4f} ({lr_train_acc*100:.2f}%)\")\n",
    "print(f\"  Validation Accuracy: {lr_val_acc:.4f} ({lr_val_acc*100:.2f}%)\")\n",
    "print(f\"  Test Accuracy: {lr_test_acc:.4f} ({lr_test_acc*100:.2f}%)\")\n",
    "\n",
    "# Detailed metrics on test set\n",
    "lr_precision = precision_score(y_test, lr_test_pred, average='weighted')\n",
    "lr_recall = recall_score(y_test, lr_test_pred, average='weighted')\n",
    "lr_f1 = f1_score(y_test, lr_test_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  Weighted Precision: {lr_precision:.4f}\")\n",
    "print(f\"  Weighted Recall: {lr_recall:.4f}\")\n",
    "print(f\"  Weighted F1-Score: {lr_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4024770",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b55ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON - TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Logistic Regression'],\n",
    "    'Accuracy': [dt_test_acc, lr_test_acc],\n",
    "    'Precision': [dt_precision, lr_precision],\n",
    "    'Recall': [dt_recall, lr_recall],\n",
    "    'F1-Score': [dt_f1, lr_f1]\n",
    "})\n",
    "\n",
    "print(\"\\n\", comparison_df.to_string(index=False))\n",
    "\n",
    "# Determine best model\n",
    "best_model = 'Logistic Regression' if lr_test_acc > dt_test_acc else 'Decision Tree'\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   Accuracy Difference: {abs(lr_test_acc - dt_test_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f71301",
   "metadata": {},
   "source": [
    "## 8. Visualizations - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5171cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Decision Tree Confusion Matrix\n",
    "dt_cm = confusion_matrix(y_test, dt_test_pred)\n",
    "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], cbar=True)\n",
    "axes[0].set_title('Decision Tree - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "\n",
    "# Logistic Regression Confusion Matrix\n",
    "lr_cm = confusion_matrix(y_test, lr_test_pred)\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1], cbar=True)\n",
    "axes[1].set_title('Logistic Regression - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_path, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrices saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00386367",
   "metadata": {},
   "source": [
    "## 9. Visualizations - Model Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79377115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy comparison across datasets\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "dt_accs = [dt_train_acc, dt_val_acc, dt_test_acc]\n",
    "lr_accs = [lr_train_acc, lr_val_acc, lr_test_acc]\n",
    "\n",
    "ax.bar(x - width/2, dt_accs, width, label='Decision Tree', alpha=0.8, color='steelblue')\n",
    "ax.bar(x + width/2, lr_accs, width, label='Logistic Regression', alpha=0.8, color='seagreen')\n",
    "\n",
    "ax.set_xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Accuracy Comparison Across Datasets', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Training', 'Validation', 'Test'])\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0.8, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (dt_acc, lr_acc) in enumerate(zip(dt_accs, lr_accs)):\n",
    "    ax.text(i - width/2, dt_acc + 0.005, f'{dt_acc:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    ax.text(i + width/2, lr_acc + 0.005, f'{lr_acc:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_path, 'accuracy_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Accuracy comparison chart saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b7d12c",
   "metadata": {},
   "source": [
    "## 10. Visualizations - Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6120c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed metrics comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "dt_values = [dt_test_acc, dt_precision, dt_recall, dt_f1]\n",
    "lr_values = [lr_test_acc, lr_precision, lr_recall, lr_f1]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, dt_values, width, label='Decision Tree', alpha=0.8, color='steelblue')\n",
    "ax.bar(x + width/2, lr_values, width, label='Logistic Regression', alpha=0.8, color='seagreen')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Test Set Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0.85, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (dt_val, lr_val) in enumerate(zip(dt_values, lr_values)):\n",
    "    ax.text(i - width/2, dt_val + 0.002, f'{dt_val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax.text(i + width/2, lr_val + 0.002, f'{lr_val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_path, 'metrics_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Metrics comparison chart saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dda053a",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3db2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 2: MACHINE LEARNING MODEL - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä DECISION TREE CLASSIFIER\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {dt_test_acc*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Max Depth: 30\")\n",
    "print(f\"   ‚Ä¢ Min Samples Split: 10\")\n",
    "print(f\"   ‚Ä¢ Interpretation: Good for non-linear patterns\")\n",
    "\n",
    "print(f\"\\nüìä LOGISTIC REGRESSION\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {lr_test_acc*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Solver: lbfgs\")\n",
    "print(f\"   ‚Ä¢ Max Iterations: 1000\")\n",
    "print(f\"   ‚Ä¢ Interpretation: Linear classifier, faster training\")\n",
    "\n",
    "print(f\"\\nüéØ BEST PERFORMING MODEL: {best_model}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {max(dt_test_acc, lr_test_acc)*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Improvement: {abs(lr_test_acc - dt_test_acc)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüíæ OUTPUT FILES GENERATED:\")\n",
    "print(f\"   ‚Ä¢ confusion_matrices.png - Confusion matrices for both models\")\n",
    "print(f\"   ‚Ä¢ accuracy_comparison.png - Accuracy across train/val/test\")\n",
    "print(f\"   ‚Ä¢ metrics_comparison.png - Detailed metrics comparison\")\n",
    "\n",
    "print(f\"\\n‚úÖ Task 2 Complete!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fab9804a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5dcd091",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2849b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed MNIST dataset...\n",
      "‚úÖ Data loaded successfully!\n",
      "\n",
      "Dataset Shapes:\n",
      "  Training: (48000, 784), Labels: (48000,)\n",
      "  Validation: (12000, 784), Labels: (12000,)\n",
      "  Test: (10000, 784), Labels: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Dataset path\n",
    "base_path = r'C:\\Users\\rushd\\OneDrive\\Documents\\leetcode'\n",
    "\n",
    "# Load preprocessed data\n",
    "print(\"Loading preprocessed MNIST dataset...\")\n",
    "X_train = np.load(os.path.join(base_path, 'X_train_scaled.npy'))\n",
    "y_train = np.load(os.path.join(base_path, 'y_train.npy'))\n",
    "X_val = np.load(os.path.join(base_path, 'X_val.npy'))\n",
    "y_val = np.load(os.path.join(base_path, 'y_val.npy'))\n",
    "X_test = np.load(os.path.join(base_path, 'X_test_scaled.npy'))\n",
    "y_test = np.load(os.path.join(base_path, 'y_test.npy'))\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "print(f\"\\nDataset Shapes:\")\n",
    "print(f\"  Training: {X_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"  Validation: {X_val.shape}, Labels: {y_val.shape}\")\n",
    "print(f\"  Test: {X_test.shape}, Labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d766a01d",
   "metadata": {},
   "source": [
    "## 3. Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "942a6c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Decision Tree Classifier...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "DecisionTreeClassifier.__init__() got an unexpected keyword argument 'n_jobs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Train Decision Tree Classifier\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining Decision Tree Classifier...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m dt_model = \u001b[43mDecisionTreeClassifier\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_samples_split\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmin_samples_leaf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m42\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m dt_model.fit(X_train, y_train)\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Decision Tree training complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: DecisionTreeClassifier.__init__() got an unexpected keyword argument 'n_jobs'"
     ]
    }
   ],
   "source": [
    "# Train Decision Tree Classifier\n",
    "print(\"Training Decision Tree Classifier...\")\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=30,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "dt_model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Decision Tree training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "dt_train_pred = dt_model.predict(X_train)\n",
    "dt_val_pred = dt_model.predict(X_val)\n",
    "dt_test_pred = dt_model.predict(X_test)\n",
    "\n",
    "print(\"‚úÖ Predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db89ffa",
   "metadata": {},
   "source": [
    "## 4. Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07b2214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    multi_class='multinomial',\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    solver='lbfgs'\n",
    ")\n",
    "\n",
    "lr_model.fit(X_train, y_train)\n",
    "print(\"‚úÖ Logistic Regression training complete!\")\n",
    "\n",
    "# Make predictions\n",
    "lr_train_pred = lr_model.predict(X_train)\n",
    "lr_val_pred = lr_model.predict(X_val)\n",
    "lr_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "print(\"‚úÖ Predictions generated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda195dd",
   "metadata": {},
   "source": [
    "## 5. Model Evaluation - Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bb40d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"DECISION TREE CLASSIFIER - PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training metrics\n",
    "dt_train_acc = accuracy_score(y_train, dt_train_pred)\n",
    "dt_val_acc = accuracy_score(y_val, dt_val_pred)\n",
    "dt_test_acc = accuracy_score(y_test, dt_test_pred)\n",
    "\n",
    "print(f\"\\nAccuracy Scores:\")\n",
    "print(f\"  Training Accuracy: {dt_train_acc:.4f} ({dt_train_acc*100:.2f}%)\")\n",
    "print(f\"  Validation Accuracy: {dt_val_acc:.4f} ({dt_val_acc*100:.2f}%)\")\n",
    "print(f\"  Test Accuracy: {dt_test_acc:.4f} ({dt_test_acc*100:.2f}%)\")\n",
    "\n",
    "# Detailed metrics on test set\n",
    "dt_precision = precision_score(y_test, dt_test_pred, average='weighted')\n",
    "dt_recall = recall_score(y_test, dt_test_pred, average='weighted')\n",
    "dt_f1 = f1_score(y_test, dt_test_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  Weighted Precision: {dt_precision:.4f}\")\n",
    "print(f\"  Weighted Recall: {dt_recall:.4f}\")\n",
    "print(f\"  Weighted F1-Score: {dt_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, dt_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81d671a",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7a0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Evaluation\n",
    "print(\"=\"*60)\n",
    "print(\"LOGISTIC REGRESSION - PERFORMANCE METRICS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training metrics\n",
    "lr_train_acc = accuracy_score(y_train, lr_train_pred)\n",
    "lr_val_acc = accuracy_score(y_val, lr_val_pred)\n",
    "lr_test_acc = accuracy_score(y_test, lr_test_pred)\n",
    "\n",
    "print(f\"\\nAccuracy Scores:\")\n",
    "print(f\"  Training Accuracy: {lr_train_acc:.4f} ({lr_train_acc*100:.2f}%)\")\n",
    "print(f\"  Validation Accuracy: {lr_val_acc:.4f} ({lr_val_acc*100:.2f}%)\")\n",
    "print(f\"  Test Accuracy: {lr_test_acc:.4f} ({lr_test_acc*100:.2f}%)\")\n",
    "\n",
    "# Detailed metrics on test set\n",
    "lr_precision = precision_score(y_test, lr_test_pred, average='weighted')\n",
    "lr_recall = recall_score(y_test, lr_test_pred, average='weighted')\n",
    "lr_f1 = f1_score(y_test, lr_test_pred, average='weighted')\n",
    "\n",
    "print(f\"\\nTest Set Metrics:\")\n",
    "print(f\"  Weighted Precision: {lr_precision:.4f}\")\n",
    "print(f\"  Weighted Recall: {lr_recall:.4f}\")\n",
    "print(f\"  Weighted F1-Score: {lr_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test, lr_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0081a5",
   "metadata": {},
   "source": [
    "## 7. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11365c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL COMPARISON - TEST SET PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Logistic Regression'],\n",
    "    'Accuracy': [dt_test_acc, lr_test_acc],\n",
    "    'Precision': [dt_precision, lr_precision],\n",
    "    'Recall': [dt_recall, lr_recall],\n",
    "    'F1-Score': [dt_f1, lr_f1]\n",
    "})\n",
    "\n",
    "print(\"\\n\", comparison_df.to_string(index=False))\n",
    "\n",
    "# Determine best model\n",
    "best_model = 'Logistic Regression' if lr_test_acc > dt_test_acc else 'Decision Tree'\n",
    "print(f\"\\nüèÜ Best Model: {best_model}\")\n",
    "print(f\"   Accuracy Difference: {abs(lr_test_acc - dt_test_acc)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e9d613",
   "metadata": {},
   "source": [
    "## 8. Visualizations - Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d5bdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Decision Tree Confusion Matrix\n",
    "dt_cm = confusion_matrix(y_test, dt_test_pred)\n",
    "sns.heatmap(dt_cm, annot=True, fmt='d', cmap='Blues', ax=axes[0], cbar=True)\n",
    "axes[0].set_title('Decision Tree - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Predicted Label')\n",
    "axes[0].set_ylabel('True Label')\n",
    "\n",
    "# Logistic Regression Confusion Matrix\n",
    "lr_cm = confusion_matrix(y_test, lr_test_pred)\n",
    "sns.heatmap(lr_cm, annot=True, fmt='d', cmap='Greens', ax=axes[1], cbar=True)\n",
    "axes[1].set_title('Logistic Regression - Confusion Matrix', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Predicted Label')\n",
    "axes[1].set_ylabel('True Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_path, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Confusion matrices saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98c982c",
   "metadata": {},
   "source": [
    "## 9. Visualizations - Model Accuracy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bafa845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy comparison across datasets\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "x = np.arange(3)\n",
    "width = 0.35\n",
    "\n",
    "dt_accs = [dt_train_acc, dt_val_acc, dt_test_acc]\n",
    "lr_accs = [lr_train_acc, lr_val_acc, lr_test_acc]\n",
    "\n",
    "ax.bar(x - width/2, dt_accs, width, label='Decision Tree', alpha=0.8, color='steelblue')\n",
    "ax.bar(x + width/2, lr_accs, width, label='Logistic Regression', alpha=0.8, color='seagreen')\n",
    "\n",
    "ax.set_xlabel('Dataset', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Model Accuracy Comparison Across Datasets', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Training', 'Validation', 'Test'])\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0.8, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, (dt_acc, lr_acc) in enumerate(zip(dt_accs, lr_accs)):\n",
    "    ax.text(i - width/2, dt_acc + 0.005, f'{dt_acc:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "    ax.text(i + width/2, lr_acc + 0.005, f'{lr_acc:.3f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_path, 'accuracy_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Accuracy comparison chart saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2a9b14",
   "metadata": {},
   "source": [
    "## 10. Visualizations - Metrics Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c285d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed metrics comparison\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "dt_values = [dt_test_acc, dt_precision, dt_recall, dt_f1]\n",
    "lr_values = [lr_test_acc, lr_precision, lr_recall, lr_f1]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "ax.bar(x - width/2, dt_values, width, label='Decision Tree', alpha=0.8, color='steelblue')\n",
    "ax.bar(x + width/2, lr_values, width, label='Logistic Regression', alpha=0.8, color='seagreen')\n",
    "\n",
    "ax.set_ylabel('Score', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Test Set Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.legend(fontsize=11)\n",
    "ax.set_ylim([0.85, 1.0])\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, (dt_val, lr_val) in enumerate(zip(dt_values, lr_values)):\n",
    "    ax.text(i - width/2, dt_val + 0.002, f'{dt_val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    ax.text(i + width/2, lr_val + 0.002, f'{lr_val:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(base_path, 'metrics_comparison.png'), dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Metrics comparison chart saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db7929",
   "metadata": {},
   "source": [
    "## 11. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f8bb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TASK 2: MACHINE LEARNING MODEL - SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(f\"\\nüìä DECISION TREE CLASSIFIER\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {dt_test_acc*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Max Depth: 30\")\n",
    "print(f\"   ‚Ä¢ Min Samples Split: 10\")\n",
    "print(f\"   ‚Ä¢ Interpretation: Good for non-linear patterns\")\n",
    "\n",
    "print(f\"\\nüìä LOGISTIC REGRESSION\")\n",
    "print(f\"   ‚Ä¢ Test Accuracy: {lr_test_acc*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Solver: lbfgs\")\n",
    "print(f\"   ‚Ä¢ Max Iterations: 1000\")\n",
    "print(f\"   ‚Ä¢ Interpretation: Linear classifier, faster training\")\n",
    "\n",
    "print(f\"\\nüéØ BEST PERFORMING MODEL: {best_model}\")\n",
    "print(f\"   ‚Ä¢ Accuracy: {max(dt_test_acc, lr_test_acc)*100:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Improvement: {abs(lr_test_acc - dt_test_acc)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüíæ OUTPUT FILES GENERATED:\")\n",
    "print(f\"   ‚Ä¢ confusion_matrices.png - Confusion matrices for both models\")\n",
    "print(f\"   ‚Ä¢ accuracy_comparison.png - Accuracy across train/val/test\")\n",
    "print(f\"   ‚Ä¢ metrics_comparison.png - Detailed metrics comparison\")\n",
    "\n",
    "print(f\"\\n‚úÖ Task 2 Complete!\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
